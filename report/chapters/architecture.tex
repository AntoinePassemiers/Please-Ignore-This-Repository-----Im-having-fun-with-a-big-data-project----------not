\chapter{Architecture}


\section{Data generation}

  \begin{figure}[ht]
    \begin{center}
      \resizebox{.85\textwidth}{!}{
        \input{chapters/generation.tex}
      }
    \end{center}
  \end{figure}


\section{Recursive Least Squares (RLS) with forgetting factor}

In the standard RLS implementation with forgetting factor,
the weights $\beta$ are estimated incrementally using the following formulas:

\begin{align}
\begin{cases}
    V^{(t)} & = \frac{1}{\nu} \Bigg( V^{(t-1)} - \frac{V^{(t-1)} x_t^T x_t V^{(t-1)}}{1 + x_t V^{(t-1)} (x_t^T} \Bigg) \\
    \alpha^{(t)} & = V^{(t)} x_t^T \\
    e & = y^{(t)} - x_t \hat{\beta}^{(t-1)} \\
    \hat{\beta}^{(t)} & = \hat{\beta}^{(t-1)} + \alpha^{(t)} e
\end{cases}
\end{align}

where 

First approach -- Fully-vectorized version:

\begin{align}
\begin{cases}
    \alpha_t & = V^{(t)} x_t^T \\
    e & = y^{(t)} - x_t \hat{B}^{(t-1)} \\
    \hat{B}^{(t)} & = \hat{B}^{(t-1)} + \alpha_t^T e
\end{cases}
\end{align}

Second approach -- Distributed version:

\begin{align}
\begin{cases}
    \alpha_t & = V^{(t)} x_t^T \\
    e & = y_j^{(t)} - x_t \hat{B}_{\cdot j}^{(t-1)} \\
    \hat{B}_{\cdot j}^{(t)} & = \hat{B}_{\cdot j}^{(t-1)} + \alpha_t^T e
\end{cases}
\end{align}


\begin{figure}[H]
    \begin{center}
        \includegraphics[width=\textwidth, keepaspectratio]{imgs/lineage-graph.png}
        \caption{Lineage graph for the proposed architecture. Dashed lines indicate
            the Spark transformations applied in the distributed version
            and plain lines indicate the transformations applied in the fully-vectorized version.}
        \label{architecture}
    \end{center}
\end{figure}


\chapter{Methodology}

\section{Data generation}

  Observations $x \in \mathbb{R}^n$ are drawn from a uniform distribution,
  while each latent coefficient vector $\beta_j \in \mathbb{R}^n$
  is drawn from a multivariate Gaussian distribution.
  Each vector $\beta_j$ is a column of the coefficient
  matrix $\beta \in \mathbb{R}^{n \times m}$ and fixed beforehand.
  Instead of choosing arbitrary mean vector and covariance
  matrix as parameters of the multivariate Gaussian distribution,
  these parameters are drawn at random. Also, the covariance matrix
  should be positive semi-definite. Therefore, a normal-Wishart
  is used to generate a random mean vector and a precision matrix.
  The final covariance matrix is obtained by inverting the sampled
  precision matrix.

  \begin{figure}[ht]
    \begin{center}
      \resizebox{.85\textwidth}{!}{
        \input{chapters/generation.tex}
      }
    \end{center}
    \caption{Bayesian network representing the randomly generated samples.
        Plate notation indicates variable repetition across time and
        output variables, respectively.}
    \label{bayesnet}
  \end{figure}

  As can be observed from the plate notation in figure \ref{bayesnet},
  coefficient vectors $\beta_j$ are computed only once for each $j$,
  and $w$, $x$, $y$ are sampled at each time step $t$, where $y_j = \beta_j x$.
  Mean vector $\mu$ and precision matrix $\Lambda$ are sampled only once
  from a normal-Wishart distribution.
  Default parameter matrix $W_0$ is the diagonal matrix, prior mean vector $\mu_0$
  is the zero vector, chosen scaling parameter is $1$, and $\nu_0$ has been
  arbitrarily set to $15$.


\section{Recursive Least Squares (RLS) with forgetting factor}

In the standard RLS implementation with forgetting factor,
the weights $\beta$ are estimated incrementally using the following formulas:

\begin{align}
\begin{cases}
    V^{(t)} & = \frac{1}{\nu} \Bigg( V^{(t-1)} - \frac{V^{(t-1)} x_t^T x_t V^{(t-1)}}{1 + x_t V^{(t-1)} (x_t^T} \Bigg) \\
    \alpha^{(t)} & = V^{(t)} x_t^T \\
    e & = y^{(t)} - x_t \hat{\beta}^{(t-1)} \\
    \hat{\beta}^{(t)} & = \hat{\beta}^{(t-1)} + \alpha^{(t)} e
\end{cases}
\end{align}

where 

First approach -- Fully-vectorized version:

\begin{align}
\begin{cases}
    \alpha_t & = V^{(t)} x_t^T \\
    e & = y^{(t)} - x_t \hat{B}^{(t-1)} \\
    \hat{B}^{(t)} & = \hat{B}^{(t-1)} + \alpha_t^T e
\end{cases}
\end{align}

Second approach -- Distributed version:

\begin{align}
\begin{cases}
    \alpha_t & = V^{(t)} x_t^T \\
    e & = y_j^{(t)} - x_t \hat{B}_{\cdot j}^{(t-1)} \\
    \hat{B}_{\cdot j}^{(t)} & = \hat{B}_{\cdot j}^{(t-1)} + \alpha_t^T e
\end{cases}
\end{align}


\begin{figure}[H]
    \begin{center}
        \includegraphics[width=\textwidth, keepaspectratio]{imgs/lineage-graph.png}
        \caption{Lineage graph for the proposed architecture. Dashed lines indicate
            the Spark transformations applied in the distributed version
            and plain lines indicate the transformations applied in the fully-vectorized version.}
        \label{architecture}
    \end{center}
\end{figure}

